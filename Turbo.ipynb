{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "В команде мы построили работу следующим образом: решили взять разные моделька каждый свою и выжать из нее максимум для задачи\n",
        "\n",
        "Моя моделька: Detectron2"
      ],
      "metadata": {
        "id": "6qwKP_Iq-Q5k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-iUNl9nH-HDC",
        "outputId": "232d258f-b66c-4a37-a91b-fa469ac8ded5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/detectron2.git@v0.6\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git (to revision v0.6) to /tmp/pip-req-build-tmnsv1g9\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-tmnsv1g9\n",
            "  Running command git checkout -q d1e04565d3bec8719335b88be9e9b961bf3ec464\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit d1e04565d3bec8719335b88be9e9b961bf3ec464\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (11.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.10.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.1.0)\n",
            "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.1.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (4.67.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.18.0)\n",
            "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (1.0.0)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.0.4)\n",
            "Requirement already satisfied: omegaconf>=2.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.3.0)\n",
            "Collecting hydra-core>=1.1 (from detectron2==0.6)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting black==21.4b2 (from detectron2==0.6)\n",
            "  Downloading black-21.4b2-py3-none-any.whl.metadata (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from black==21.4b2->detectron2==0.6) (8.2.1)\n",
            "Collecting appdirs (from black==21.4b2->detectron2==0.6)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: toml>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from black==21.4b2->detectron2==0.6) (0.10.2)\n",
            "Requirement already satisfied: regex>=2020.1.8 in /usr/local/lib/python3.11/dist-packages (from black==21.4b2->detectron2==0.6) (2024.11.6)\n",
            "Collecting pathspec<1,>=0.8.1 (from black==21.4b2->detectron2==0.6)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting mypy-extensions>=0.4.3 (from black==21.4b2->detectron2==0.6)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.1->detectron2==0.6) (24.2)\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (2.9.0.post0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (3.8)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (3.0.2)\n",
            "Downloading black-21.4b2-py3-none-any.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: detectron2, fvcore\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp311-cp311-linux_x86_64.whl size=6418104 sha256=ff6daa987a3c9876778cdc2bcd52a2948a7c1690baeabf16c498848042db6919\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8hrw9_lb/wheels/ca/39/ee/b22abcd225d0332c6ed94841288b9ece53edfd7aa2c199479c\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=d6f110507595e7f57843b3223adf3790f314ac83d0fcc1bb7d0d2bbbf5d35eb6\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n",
            "Successfully built detectron2 fvcore\n",
            "Installing collected packages: appdirs, yacs, portalocker, pathspec, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n",
            "Successfully installed appdirs-1.4.4 black-21.4b2 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.1.0 pathspec-0.12.1 portalocker-3.1.1 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/facebookresearch/detectron2.git@v0.6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y Pillow\n",
        "!pip install Pillow==8.4.0\n",
        "import site\n",
        "import sys\n",
        "from importlib import reload\n",
        "reload(site)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "collapsed": true,
        "id": "v6wKP_1J-WOP",
        "outputId": "0245fa56-67d1-4625-e13d-d8e77cc683de"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping Pillow as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting Pillow==8.4.0\n",
            "  Using cached Pillow-8.4.0.tar.gz (49.4 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: Pillow\n",
            "  Building wheel for Pillow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Pillow: filename=Pillow-8.4.0-cp311-cp311-linux_x86_64.whl size=1171652 sha256=151097aa6acdc99dcdfa014f3a65eeda1264a593844df3e04fc444d8b13aec75\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/cf/6a/49767496ca114325703308efc0519f97126e3ca06dd91845b7\n",
            "Successfully built Pillow\n",
            "Installing collected packages: Pillow\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scikit-image 0.25.2 requires pillow>=10.1, but you have pillow 8.4.0 which is incompatible.\n",
            "torchtune 0.6.1 requires Pillow>=9.4.0, but you have pillow 8.4.0 which is incompatible.\n",
            "fastai 2.7.19 requires pillow>=9.0.0, but you have pillow 8.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-8.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "053c5357eb564cc4b23606cab2a8e70d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'site' (frozen)>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "print(f\"Detectron2 version: {detectron2.__version__}\")\n",
        "print(\"Установка прошла успешно!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU5BVgE6-Xsg",
        "outputId": "7336dac5-cfc4-446e-af08-5ce163b8229c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detectron2 version: 0.6\n",
            "Установка прошла успешно!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_train_loader, build_detection_test_loader\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.data import detection_utils as utils\n",
        "from detectron2.data import transforms as T\n",
        "from detectron2.data import DatasetMapper\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2"
      ],
      "metadata": {
        "id": "pFTM8jJIIWfK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "from detectron2.data.datasets import register_coco_instances"
      ],
      "metadata": {
        "id": "ls6n-bogGgT9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf4ujjazGoqg",
        "outputId": "b288a120-e434-42d3-ce7f-d734e0a5a04a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "classes = [\"damage\", \"corrosion\", \"lightning\", \"lightning receptor\", \"missing teeth\", \"patch\"]\n",
        "\n",
        "for name in [\"train_dataset\", \"val_dataset\"]:\n",
        "    if name in DatasetCatalog.list():\n",
        "        DatasetCatalog.remove(name)\n",
        "        MetadataCatalog.remove(name)\n",
        "\n",
        "register_coco_instances(\"train_dataset\", {}, \"/content/drive/MyDrive/Wind Turbine damage/train/_annotations.coco.json\", \"/content/Wind Turbine damage/train/\")\n",
        "register_coco_instances(\"val_dataset\", {}, \"/content/drive/MyDrive/Wind Turbine damage/valid/_annotations.coco.json\", \"/content/Wind Turbine damage/valid/\")\n",
        "\n",
        "MetadataCatalog.get(\"train_dataset\").thing_classes = classes\n",
        "MetadataCatalog.get(\"val_dataset\").thing_classes = classes\n",
        "\n",
        "dataset_name = \"train_dataset\"\n",
        "dataset_dicts = DatasetCatalog.get(dataset_name)\n",
        "metadata = MetadataCatalog.get(dataset_name)\n",
        "\n",
        "num_images_to_visualize = 5\n",
        "\n",
        "for d in random.sample(dataset_dicts, num_images_to_visualize):\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    if img is None:\n",
        "        print(f\"Ошибка: Не удалось загрузить изображение {d['file_name']}\")\n",
        "        continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuTxL-M6--oh",
        "outputId": "daff1bc5-1ca2-4f99-f40b-8a3fea443af3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/03 18:43:37 d2.data.datasets.coco]: Loading /content/drive/MyDrive/Wind Turbine damage/train/_annotations.coco.json takes 4.31 seconds.\n",
            "WARNING [06/03 18:43:37 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[06/03 18:43:37 d2.data.datasets.coco]: Loaded 1070 images in COCO format from /content/drive/MyDrive/Wind Turbine damage/train/_annotations.coco.json\n",
            "Ошибка: Не удалось загрузить изображение /content/Wind Turbine damage/train/DJI_0029_JPG.rf.c868e504f0d460ff990dae886f6a551d.jpg\n",
            "Ошибка: Не удалось загрузить изображение /content/Wind Turbine damage/train/20240120_163647_jpg.rf.d609884c32b1a5637fd12c9db78760ff.jpg\n",
            "Ошибка: Не удалось загрузить изображение /content/Wind Turbine damage/train/20240120_155211_mp4-0092_jpg.rf.3d902d08081341e1ad3abb7081d9ccbd.jpg\n",
            "Ошибка: Не удалось загрузить изображение /content/Wind Turbine damage/train/DJI_0598-1-_JPG.rf.daa4e2b20acb1f41b2677250c18d6f4e.jpg\n",
            "Ошибка: Не удалось загрузить изображение /content/Wind Turbine damage/train/20240120_153811_jpg.rf.0ceaa485ba4d1e6bcb42f50b77ab3071.jpg\n",
            "Завершено. Проверьте папку './visualization_output' для просмотра визуализированных изображений.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\"damage\",\"corrosion\", \"lightning\", \"lightning receptor\", \"missing teeth\", \"patch\"]\n",
        "\n",
        "for name in [\"train_dataset\", \"val_dataset\"]:\n",
        "    if name in DatasetCatalog.list():\n",
        "        DatasetCatalog.remove(name)\n",
        "        MetadataCatalog.remove(name)\n",
        "\n",
        "train_json_path = \"/content/drive/MyDrive/Wind Turbine damage/train/_annotations.coco.json\"\n",
        "train_image_dir = \"/content/drive/MyDrive/Wind Turbine damage/train/\"\n",
        "val_json_path = \"/content/drive/MyDrive/Wind Turbine damage/valid/_annotations.coco.json\"\n",
        "val_image_dir = \"/content/drive/MyDrive/Wind Turbine damage/valid/\"\n",
        "\n",
        "if not os.path.exists(train_json_path):\n",
        "    print(f\"Ошибка: Файл аннотаций для обучения не найден: {train_json_path}\")\n",
        "\n",
        "if not os.path.exists(val_json_path):\n",
        "    print(f\"Ошибка: Файл аннотаций для валидации не найден: {val_json_path}\")\n",
        "\n",
        "register_coco_instances(\"train_dataset\", {}, train_json_path, train_image_dir)\n",
        "register_coco_instances(\"val_dataset\", {}, val_json_path, val_image_dir)\n",
        "\n",
        "MetadataCatalog.get(\"train_dataset\").thing_classes = classes\n",
        "MetadataCatalog.get(\"val_dataset\").thing_classes = classes"
      ],
      "metadata": {
        "id": "p6TqygnTHFul"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDatasetMapper(DatasetMapper):\n",
        "    def __init__(self, cfg, is_train=True):\n",
        "        super().__init__(cfg, is_train)\n",
        "        self.augmentations = [\n",
        "            T.ResizeShortestEdge(\n",
        "                cfg.INPUT.MIN_SIZE_TRAIN,\n",
        "                cfg.INPUT.MAX_SIZE_TRAIN,\n",
        "                cfg.INPUT.MIN_SIZE_TRAIN_SAMPLING\n",
        "            ),\n",
        "            T.RandomFlip(horizontal=True)\n",
        "        ]\n",
        "        if is_train:\n",
        "            self.augmentations.extend([\n",
        "            ])\n",
        "\n",
        "    def __call__(self, dataset_dict):\n",
        "        dataset_dict = dataset_dict.copy()\n",
        "        image = utils.read_image(dataset_dict[\"file_name\"], format=self.image_format)\n",
        "\n",
        "        aug_input = T.AugInput(image)\n",
        "        transforms = T.AugmentationList(self.augmentations)(aug_input)\n",
        "        image = aug_input.image\n",
        "\n",
        "        annos = [\n",
        "            utils.transform_instance_annotations(obj, transforms, image.shape[:2])\n",
        "            for obj in dataset_dict.pop(\"annotations\")\n",
        "        ]\n",
        "        valid_annos = []\n",
        "        for anno in annos:\n",
        "            x1, y1, w, h = anno[\"bbox\"]\n",
        "            if w > 0 and h > 0 and x1 >= 0 and y1 >= 0 and x1 + w <= image.shape[1] and y1 + h <= image.shape[0]:\n",
        "                valid_annos.append(anno)\n",
        "\n",
        "        return {\n",
        "            \"image\": torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\")),\n",
        "            \"instances\": utils.annotations_to_instances(valid_annos, image.shape[:2]),\n",
        "            \"height\": image.shape[0],\n",
        "            \"width\": image.shape[1],\n",
        "        }"
      ],
      "metadata": {
        "id": "_PF1xHR0IJp_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTrainer(DefaultTrainer):\n",
        "    @classmethod\n",
        "    def build_train_loader(cls, cfg):\n",
        "        return build_detection_train_loader(cfg, mapper=CustomDatasetMapper(cfg, is_train=True))\n",
        "\n",
        "    @classmethod\n",
        "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
        "        if output_folder is None:\n",
        "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
        "        return COCOEvaluator(dataset_name, output_dir=output_folder)\n",
        "\n",
        "# --- 5. Конфигурация модели и параметров обучения ---\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Используемое устройство: {cfg.MODEL.DEVICE}\")\n",
        "\n",
        "# Датасеты для обучения и тестирования\n",
        "cfg.DATASETS.TRAIN = (\"train_dataset\",)\n",
        "cfg.DATASETS.TEST = (\"val_dataset\",) # Используем наш валидационный датасет для оценки\n",
        "cfg.DATALOADER.NUM_WORKERS = 2 # 2-4 обычно хорошее значение\n",
        "cfg.INPUT.MIN_SIZE_TRAIN = (640, 672, 704, 736, 768, 800)\n",
        "cfg.INPUT.MAX_SIZE_TRAIN = 1333\n",
        "cfg.INPUT.MIN_SIZE_TRAIN_SAMPLING = \"choice\" # Случайный выбор из MIN_SIZE_TRAIN\n",
        "\n",
        "cfg.INPUT.MIN_SIZE_TEST = 800\n",
        "cfg.INPUT.MAX_SIZE_TEST = 1333\n",
        "\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(classes)\n",
        "\n",
        "\n",
        "cfg.SOLVER.IMS_PER_BATCH = 4\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 1500\n",
        "cfg.SOLVER.STEPS = (10000, 13000)\n",
        "cfg.SOLVER.GAMMA = 0.1\n",
        "cfg.TEST.EVAL_PERIOD = 1000\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XU7Dv9jIdh5",
        "outputId": "334b86ac-d5ac-4369-9b69-6e2bc812e65c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Используемое устройство: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.OUTPUT_DIR = \"./output_wind_turbine_damage\"\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Начало обучения...\")\n",
        "trainer = CustomTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()\n",
        "print(\"Обучение завершено.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AF5NGsvEI8EU",
        "outputId": "5cc4c28b-07a2-476e-c939-5d6b707c825c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Начало обучения...\n",
            "[06/03 20:33:42 d2.engine.defaults]: Model:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=7, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=24, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "[06/03 20:33:42 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "WARNING [06/03 20:33:42 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[06/03 20:33:42 d2.data.datasets.coco]: Loaded 1070 images in COCO format from /content/drive/MyDrive/Wind Turbine damage/train/_annotations.coco.json\n",
            "[06/03 20:33:42 d2.data.build]: Removed 316 images with no usable annotations. 754 images left.\n",
            "[06/03 20:33:42 d2.data.build]: Using training sampler TrainingSampler\n",
            "[06/03 20:33:42 d2.data.common]: Serializing 754 elements to byte tensors and concatenating them all ...\n",
            "[06/03 20:33:42 d2.data.common]: Serialized dataset takes 0.60 MiB\n",
            "WARNING [06/03 20:33:42 d2.solver.build]: SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (7, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (7,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (24, 1024) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (24,) in the model! You might want to double check if this is expected.\n",
            "WARNING:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:\n",
            "roi_heads.box_predictor.bbox_pred.{bias, weight}\n",
            "roi_heads.box_predictor.cls_score.{bias, weight}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/03 20:33:43 d2.engine.train_loop]: Starting training from iteration 0\n",
            "[06/03 20:34:25 d2.utils.events]:  eta: 0:41:36  iter: 19  total_loss: 5.041  loss_cls: 2.019  loss_box_reg: 0.000435  loss_rpn_cls: 2.511  loss_rpn_loc: 0.4273  time: 1.9269  data_time: 0.9743  lr: 4.9953e-06  max_mem: 5030M\n",
            "[06/03 20:35:05 d2.utils.events]:  eta: 0:41:02  iter: 39  total_loss: 4.229  loss_cls: 1.784  loss_box_reg: 0.003677  loss_rpn_cls: 1.933  loss_rpn_loc: 0.4374  time: 1.9678  data_time: 1.0028  lr: 9.9902e-06  max_mem: 5030M\n",
            "[06/03 20:35:47 d2.utils.events]:  eta: 0:42:55  iter: 59  total_loss: 2.491  loss_cls: 1.359  loss_box_reg: 0.002158  loss_rpn_cls: 0.8653  loss_rpn_loc: 0.4063  time: 2.0110  data_time: 1.0262  lr: 1.4985e-05  max_mem: 5030M\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:2918: DecompressionBombWarning: Image size (149817600 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/03 20:36:33 d2.utils.events]:  eta: 0:42:45  iter: 79  total_loss: 1.401  loss_cls: 0.846  loss_box_reg: 0.001746  loss_rpn_cls: 0.3404  loss_rpn_loc: 0.1817  time: 2.0911  data_time: 1.3119  lr: 1.998e-05  max_mem: 5030M\n",
            "[06/03 20:37:10 d2.utils.events]:  eta: 0:42:47  iter: 99  total_loss: 0.7332  loss_cls: 0.2719  loss_box_reg: 0.003132  loss_rpn_cls: 0.137  loss_rpn_loc: 0.2872  time: 2.0402  data_time: 0.8880  lr: 2.4975e-05  max_mem: 5030M\n",
            "[06/03 20:37:52 d2.utils.events]:  eta: 0:41:33  iter: 119  total_loss: 0.5805  loss_cls: 0.1309  loss_box_reg: 0.001102  loss_rpn_cls: 0.1532  loss_rpn_loc: 0.2418  time: 2.0507  data_time: 1.1128  lr: 2.997e-05  max_mem: 5030M\n",
            "[06/03 20:38:34 d2.utils.events]:  eta: 0:40:56  iter: 139  total_loss: 0.4809  loss_cls: 0.08598  loss_box_reg: 0.004648  loss_rpn_cls: 0.1534  loss_rpn_loc: 0.2437  time: 2.0600  data_time: 1.1212  lr: 3.4965e-05  max_mem: 5030M\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:2918: DecompressionBombWarning: Image size (149817600 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/03 20:39:17 d2.utils.events]:  eta: 0:40:20  iter: 159  total_loss: 0.4802  loss_cls: 0.09008  loss_box_reg: 0.005441  loss_rpn_cls: 0.1219  loss_rpn_loc: 0.2428  time: 2.0666  data_time: 1.1235  lr: 3.996e-05  max_mem: 5030M\n",
            "[06/03 20:39:55 d2.utils.events]:  eta: 0:39:45  iter: 179  total_loss: 0.4387  loss_cls: 0.07633  loss_box_reg: 0.01148  loss_rpn_cls: 0.1209  loss_rpn_loc: 0.2168  time: 2.0524  data_time: 0.9087  lr: 4.4955e-05  max_mem: 5030M\n",
            "[06/03 20:40:35 d2.utils.events]:  eta: 0:39:44  iter: 199  total_loss: 0.4236  loss_cls: 0.05699  loss_box_reg: 0.006574  loss_rpn_cls: 0.1384  loss_rpn_loc: 0.2012  time: 2.0427  data_time: 1.0197  lr: 4.995e-05  max_mem: 5030M\n",
            "[06/03 20:41:16 d2.utils.events]:  eta: 0:39:33  iter: 219  total_loss: 0.5433  loss_cls: 0.09914  loss_box_reg: 0.01793  loss_rpn_cls: 0.113  loss_rpn_loc: 0.2684  time: 2.0439  data_time: 1.0366  lr: 5.4945e-05  max_mem: 5030M\n",
            "[06/03 20:41:55 d2.utils.events]:  eta: 0:39:30  iter: 239  total_loss: 0.4462  loss_cls: 0.07404  loss_box_reg: 0.008783  loss_rpn_cls: 0.1117  loss_rpn_loc: 0.2443  time: 2.0375  data_time: 0.9682  lr: 5.994e-05  max_mem: 5030M\n",
            "[06/03 20:42:34 d2.utils.events]:  eta: 0:38:20  iter: 259  total_loss: 0.4138  loss_cls: 0.06925  loss_box_reg: 0.01116  loss_rpn_cls: 0.1018  loss_rpn_loc: 0.2153  time: 2.0315  data_time: 0.9552  lr: 6.4935e-05  max_mem: 5030M\n",
            "[06/03 20:43:13 d2.utils.events]:  eta: 0:37:41  iter: 279  total_loss: 0.4406  loss_cls: 0.06422  loss_box_reg: 0.01145  loss_rpn_cls: 0.09545  loss_rpn_loc: 0.2148  time: 2.0240  data_time: 0.9297  lr: 6.993e-05  max_mem: 5030M\n",
            "[06/03 20:43:55 d2.utils.events]:  eta: 0:37:37  iter: 299  total_loss: 0.3092  loss_cls: 0.07008  loss_box_reg: 0.01126  loss_rpn_cls: 0.08637  loss_rpn_loc: 0.1526  time: 2.0280  data_time: 1.0936  lr: 7.4925e-05  max_mem: 5030M\n",
            "[06/03 20:44:36 d2.utils.events]:  eta: 0:36:59  iter: 319  total_loss: 0.3252  loss_cls: 0.0577  loss_box_reg: 0.02005  loss_rpn_cls: 0.0673  loss_rpn_loc: 0.1305  time: 2.0316  data_time: 1.0869  lr: 7.992e-05  max_mem: 5030M\n",
            "[06/03 20:45:21 d2.utils.events]:  eta: 0:35:53  iter: 339  total_loss: 0.4745  loss_cls: 0.08495  loss_box_reg: 0.01778  loss_rpn_cls: 0.09369  loss_rpn_loc: 0.2642  time: 2.0435  data_time: 1.2208  lr: 8.4915e-05  max_mem: 5030M\n",
            "[06/03 20:46:06 d2.utils.events]:  eta: 0:35:02  iter: 359  total_loss: 0.4437  loss_cls: 0.08253  loss_box_reg: 0.01527  loss_rpn_cls: 0.08824  loss_rpn_loc: 0.2273  time: 2.0564  data_time: 1.2772  lr: 8.991e-05  max_mem: 5030M\n",
            "[06/03 20:46:46 d2.utils.events]:  eta: 0:34:04  iter: 379  total_loss: 0.2843  loss_cls: 0.05716  loss_box_reg: 0.01276  loss_rpn_cls: 0.08791  loss_rpn_loc: 0.1643  time: 2.0521  data_time: 0.9257  lr: 9.4905e-05  max_mem: 5030M\n",
            "[06/03 20:47:25 d2.utils.events]:  eta: 0:33:17  iter: 399  total_loss: 0.3131  loss_cls: 0.06031  loss_box_reg: 0.02442  loss_rpn_cls: 0.06544  loss_rpn_loc: 0.1453  time: 2.0483  data_time: 0.9588  lr: 9.99e-05  max_mem: 5030M\n",
            "[06/03 20:48:04 d2.utils.events]:  eta: 0:32:35  iter: 419  total_loss: 0.4233  loss_cls: 0.07629  loss_box_reg: 0.02066  loss_rpn_cls: 0.09029  loss_rpn_loc: 0.2423  time: 2.0427  data_time: 0.9358  lr: 0.0001049  max_mem: 5030M\n",
            "[06/03 20:48:43 d2.utils.events]:  eta: 0:32:04  iter: 439  total_loss: 0.3199  loss_cls: 0.05423  loss_box_reg: 0.009474  loss_rpn_cls: 0.0746  loss_rpn_loc: 0.1879  time: 2.0380  data_time: 0.9294  lr: 0.00010989  max_mem: 5030M\n",
            "[06/03 20:49:22 d2.utils.events]:  eta: 0:31:28  iter: 459  total_loss: 0.4501  loss_cls: 0.08241  loss_box_reg: 0.02977  loss_rpn_cls: 0.07196  loss_rpn_loc: 0.2814  time: 2.0352  data_time: 0.9760  lr: 0.00011489  max_mem: 5030M\n",
            "[06/03 20:50:03 d2.utils.events]:  eta: 0:31:07  iter: 479  total_loss: 0.2964  loss_cls: 0.0643  loss_box_reg: 0.02473  loss_rpn_cls: 0.07935  loss_rpn_loc: 0.1242  time: 2.0345  data_time: 1.0144  lr: 0.00011988  max_mem: 5030M\n",
            "[06/03 20:50:49 d2.utils.events]:  eta: 0:30:37  iter: 499  total_loss: 0.3598  loss_cls: 0.0721  loss_box_reg: 0.01841  loss_rpn_cls: 0.05543  loss_rpn_loc: 0.1813  time: 2.0463  data_time: 1.2634  lr: 0.00012488  max_mem: 5030M\n",
            "[06/03 20:51:31 d2.utils.events]:  eta: 0:30:09  iter: 519  total_loss: 0.3809  loss_cls: 0.0672  loss_box_reg: 0.01994  loss_rpn_cls: 0.06659  loss_rpn_loc: 0.2156  time: 2.0483  data_time: 1.0715  lr: 0.00012987  max_mem: 5030M\n",
            "[06/03 20:52:18 d2.utils.events]:  eta: 0:29:38  iter: 539  total_loss: 0.4068  loss_cls: 0.06827  loss_box_reg: 0.02156  loss_rpn_cls: 0.07945  loss_rpn_loc: 0.2039  time: 2.0582  data_time: 1.2997  lr: 0.00013487  max_mem: 5030M\n",
            "[06/03 20:53:00 d2.utils.events]:  eta: 0:29:01  iter: 559  total_loss: 0.4785  loss_cls: 0.06516  loss_box_reg: 0.0179  loss_rpn_cls: 0.07707  loss_rpn_loc: 0.2572  time: 2.0597  data_time: 1.0802  lr: 0.00013986  max_mem: 5030M\n",
            "[06/03 20:53:40 d2.utils.events]:  eta: 0:28:25  iter: 579  total_loss: 0.3169  loss_cls: 0.06619  loss_box_reg: 0.02032  loss_rpn_cls: 0.05788  loss_rpn_loc: 0.1749  time: 2.0577  data_time: 0.9343  lr: 0.00014486  max_mem: 5030M\n",
            "[06/03 20:54:19 d2.utils.events]:  eta: 0:27:48  iter: 599  total_loss: 0.3549  loss_cls: 0.06042  loss_box_reg: 0.01863  loss_rpn_cls: 0.07703  loss_rpn_loc: 0.1712  time: 2.0543  data_time: 0.9585  lr: 0.00014985  max_mem: 5030M\n",
            "[06/03 20:54:59 d2.utils.events]:  eta: 0:27:11  iter: 619  total_loss: 0.3311  loss_cls: 0.06392  loss_box_reg: 0.02082  loss_rpn_cls: 0.06098  loss_rpn_loc: 0.1767  time: 2.0529  data_time: 0.9899  lr: 0.00015485  max_mem: 5030M\n",
            "[06/03 20:55:38 d2.utils.events]:  eta: 0:26:34  iter: 639  total_loss: 0.378  loss_cls: 0.07809  loss_box_reg: 0.03313  loss_rpn_cls: 0.06621  loss_rpn_loc: 0.2084  time: 2.0501  data_time: 0.9434  lr: 0.00015984  max_mem: 5030M\n",
            "[06/03 20:56:18 d2.utils.events]:  eta: 0:25:56  iter: 659  total_loss: 0.3486  loss_cls: 0.0689  loss_box_reg: 0.02399  loss_rpn_cls: 0.05957  loss_rpn_loc: 0.1826  time: 2.0475  data_time: 0.9236  lr: 0.00016484  max_mem: 5030M\n",
            "[06/03 20:56:56 d2.utils.events]:  eta: 0:25:19  iter: 679  total_loss: 0.3456  loss_cls: 0.07195  loss_box_reg: 0.02439  loss_rpn_cls: 0.05594  loss_rpn_loc: 0.1777  time: 2.0428  data_time: 0.8926  lr: 0.00016983  max_mem: 5030M\n",
            "[06/03 20:57:37 d2.utils.events]:  eta: 0:24:41  iter: 699  total_loss: 0.359  loss_cls: 0.05892  loss_box_reg: 0.02879  loss_rpn_cls: 0.05603  loss_rpn_loc: 0.1891  time: 2.0440  data_time: 1.0441  lr: 0.00017483  max_mem: 5030M\n",
            "[06/03 20:58:17 d2.utils.events]:  eta: 0:24:05  iter: 719  total_loss: 0.2491  loss_cls: 0.05108  loss_box_reg: 0.01446  loss_rpn_cls: 0.05326  loss_rpn_loc: 0.1071  time: 2.0430  data_time: 0.9898  lr: 0.00017982  max_mem: 5030M\n",
            "[06/03 20:58:56 d2.utils.events]:  eta: 0:23:28  iter: 739  total_loss: 0.3916  loss_cls: 0.07076  loss_box_reg: 0.0294  loss_rpn_cls: 0.05387  loss_rpn_loc: 0.1939  time: 2.0395  data_time: 0.9161  lr: 0.00018482  max_mem: 5030M\n",
            "[06/03 20:59:48 d2.utils.events]:  eta: 0:22:51  iter: 759  total_loss: 0.3934  loss_cls: 0.07164  loss_box_reg: 0.02478  loss_rpn_cls: 0.04949  loss_rpn_loc: 0.2103  time: 2.0542  data_time: 1.5474  lr: 0.00018981  max_mem: 5030M\n",
            "[06/03 21:00:24 d2.utils.events]:  eta: 0:22:13  iter: 779  total_loss: 0.3207  loss_cls: 0.0573  loss_box_reg: 0.02342  loss_rpn_cls: 0.05419  loss_rpn_loc: 0.161  time: 2.0486  data_time: 0.8047  lr: 0.00019481  max_mem: 5030M\n",
            "[06/03 21:01:06 d2.utils.events]:  eta: 0:21:36  iter: 799  total_loss: 0.27  loss_cls: 0.05683  loss_box_reg: 0.02654  loss_rpn_cls: 0.04489  loss_rpn_loc: 0.1429  time: 2.0499  data_time: 1.0820  lr: 0.0001998  max_mem: 5030M\n",
            "[06/03 21:01:45 d2.utils.events]:  eta: 0:20:59  iter: 819  total_loss: 0.4014  loss_cls: 0.06999  loss_box_reg: 0.02162  loss_rpn_cls: 0.05357  loss_rpn_loc: 0.2381  time: 2.0475  data_time: 0.9303  lr: 0.0002048  max_mem: 5030M\n",
            "[06/03 21:02:31 d2.utils.events]:  eta: 0:20:22  iter: 839  total_loss: 0.432  loss_cls: 0.07917  loss_box_reg: 0.0274  loss_rpn_cls: 0.05046  loss_rpn_loc: 0.2373  time: 2.0534  data_time: 1.2939  lr: 0.00020979  max_mem: 5030M\n",
            "[06/03 21:03:13 d2.utils.events]:  eta: 0:19:45  iter: 859  total_loss: 0.3825  loss_cls: 0.07439  loss_box_reg: 0.0313  loss_rpn_cls: 0.04756  loss_rpn_loc: 0.2026  time: 2.0540  data_time: 1.0298  lr: 0.00021479  max_mem: 5030M\n",
            "[06/03 21:03:51 d2.utils.events]:  eta: 0:19:08  iter: 879  total_loss: 0.355  loss_cls: 0.06587  loss_box_reg: 0.02692  loss_rpn_cls: 0.03673  loss_rpn_loc: 0.1738  time: 2.0503  data_time: 0.9169  lr: 0.00021978  max_mem: 5030M\n",
            "[06/03 21:04:36 d2.utils.events]:  eta: 0:18:31  iter: 899  total_loss: 0.3306  loss_cls: 0.07205  loss_box_reg: 0.03764  loss_rpn_cls: 0.05357  loss_rpn_loc: 0.1449  time: 2.0548  data_time: 1.2527  lr: 0.00022478  max_mem: 5030M\n",
            "[06/03 21:05:14 d2.utils.events]:  eta: 0:17:54  iter: 919  total_loss: 0.432  loss_cls: 0.08085  loss_box_reg: 0.02939  loss_rpn_cls: 0.05973  loss_rpn_loc: 0.2413  time: 2.0520  data_time: 0.8763  lr: 0.00022977  max_mem: 5030M\n",
            "[06/03 21:05:55 d2.utils.events]:  eta: 0:17:19  iter: 939  total_loss: 0.3278  loss_cls: 0.06834  loss_box_reg: 0.0261  loss_rpn_cls: 0.04735  loss_rpn_loc: 0.1509  time: 2.0515  data_time: 0.9947  lr: 0.00023477  max_mem: 5030M\n",
            "[06/03 21:06:38 d2.utils.events]:  eta: 0:16:45  iter: 959  total_loss: 0.2651  loss_cls: 0.05966  loss_box_reg: 0.02991  loss_rpn_cls: 0.04139  loss_rpn_loc: 0.1417  time: 2.0533  data_time: 1.0952  lr: 0.00023976  max_mem: 5030M\n",
            "[06/03 21:07:28 d2.utils.events]:  eta: 0:16:10  iter: 979  total_loss: 0.3272  loss_cls: 0.04894  loss_box_reg: 0.0232  loss_rpn_cls: 0.05143  loss_rpn_loc: 0.1864  time: 2.0623  data_time: 1.4600  lr: 0.00024476  max_mem: 5030M\n",
            "WARNING [06/03 21:08:05 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[06/03 21:08:05 d2.data.datasets.coco]: Loaded 271 images in COCO format from /content/drive/MyDrive/Wind Turbine damage/valid/_annotations.coco.json\n",
            "[06/03 21:08:05 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[06/03 21:08:05 d2.data.common]: Serializing 271 elements to byte tensors and concatenating them all ...\n",
            "[06/03 21:08:05 d2.data.common]: Serialized dataset takes 0.18 MiB\n",
            "[06/03 21:08:06 d2.evaluation.evaluator]: Start inference on 271 batches\n",
            "[06/03 21:08:13 d2.evaluation.evaluator]: Inference done 11/271. Dataloading: 0.2647 s/iter. Inference: 0.1098 s/iter. Eval: 0.0003 s/iter. Total: 0.3748 s/iter. ETA=0:01:37\n",
            "[06/03 21:08:18 d2.evaluation.evaluator]: Inference done 20/271. Dataloading: 0.3742 s/iter. Inference: 0.1088 s/iter. Eval: 0.0004 s/iter. Total: 0.4837 s/iter. ETA=0:02:01\n",
            "[06/03 21:08:23 d2.evaluation.evaluator]: Inference done 26/271. Dataloading: 0.4744 s/iter. Inference: 0.1137 s/iter. Eval: 0.0004 s/iter. Total: 0.5889 s/iter. ETA=0:02:24\n",
            "[06/03 21:08:28 d2.evaluation.evaluator]: Inference done 37/271. Dataloading: 0.4347 s/iter. Inference: 0.1135 s/iter. Eval: 0.0006 s/iter. Total: 0.5494 s/iter. ETA=0:02:08\n",
            "[06/03 21:08:35 d2.evaluation.evaluator]: Inference done 51/271. Dataloading: 0.4154 s/iter. Inference: 0.1147 s/iter. Eval: 0.0005 s/iter. Total: 0.5311 s/iter. ETA=0:01:56\n",
            "[06/03 21:08:40 d2.evaluation.evaluator]: Inference done 60/271. Dataloading: 0.4179 s/iter. Inference: 0.1173 s/iter. Eval: 0.0005 s/iter. Total: 0.5362 s/iter. ETA=0:01:53\n",
            "[06/03 21:08:45 d2.evaluation.evaluator]: Inference done 67/271. Dataloading: 0.4392 s/iter. Inference: 0.1177 s/iter. Eval: 0.0004 s/iter. Total: 0.5579 s/iter. ETA=0:01:53\n",
            "[06/03 21:08:51 d2.evaluation.evaluator]: Inference done 84/271. Dataloading: 0.3868 s/iter. Inference: 0.1159 s/iter. Eval: 0.0005 s/iter. Total: 0.5037 s/iter. ETA=0:01:34\n",
            "[06/03 21:08:56 d2.evaluation.evaluator]: Inference done 93/271. Dataloading: 0.3988 s/iter. Inference: 0.1160 s/iter. Eval: 0.0005 s/iter. Total: 0.5157 s/iter. ETA=0:01:31\n",
            "[06/03 21:09:02 d2.evaluation.evaluator]: Inference done 109/271. Dataloading: 0.3743 s/iter. Inference: 0.1150 s/iter. Eval: 0.0005 s/iter. Total: 0.4902 s/iter. ETA=0:01:19\n",
            "[06/03 21:09:07 d2.evaluation.evaluator]: Inference done 119/271. Dataloading: 0.3745 s/iter. Inference: 0.1163 s/iter. Eval: 0.0005 s/iter. Total: 0.4917 s/iter. ETA=0:01:14\n",
            "[06/03 21:09:12 d2.evaluation.evaluator]: Inference done 142/271. Dataloading: 0.3307 s/iter. Inference: 0.1146 s/iter. Eval: 0.0004 s/iter. Total: 0.4461 s/iter. ETA=0:00:57\n",
            "[06/03 21:09:17 d2.evaluation.evaluator]: Inference done 158/271. Dataloading: 0.3185 s/iter. Inference: 0.1135 s/iter. Eval: 0.0004 s/iter. Total: 0.4327 s/iter. ETA=0:00:48\n",
            "[06/03 21:09:22 d2.evaluation.evaluator]: Inference done 167/271. Dataloading: 0.3264 s/iter. Inference: 0.1139 s/iter. Eval: 0.0004 s/iter. Total: 0.4411 s/iter. ETA=0:00:45\n",
            "[06/03 21:09:27 d2.evaluation.evaluator]: Inference done 185/271. Dataloading: 0.3105 s/iter. Inference: 0.1138 s/iter. Eval: 0.0004 s/iter. Total: 0.4251 s/iter. ETA=0:00:36\n",
            "[06/03 21:09:33 d2.evaluation.evaluator]: Inference done 210/271. Dataloading: 0.2860 s/iter. Inference: 0.1129 s/iter. Eval: 0.0005 s/iter. Total: 0.3998 s/iter. ETA=0:00:24\n",
            "[06/03 21:09:38 d2.evaluation.evaluator]: Inference done 225/271. Dataloading: 0.2833 s/iter. Inference: 0.1134 s/iter. Eval: 0.0005 s/iter. Total: 0.3975 s/iter. ETA=0:00:18\n",
            "[06/03 21:09:43 d2.evaluation.evaluator]: Inference done 248/271. Dataloading: 0.2684 s/iter. Inference: 0.1120 s/iter. Eval: 0.0005 s/iter. Total: 0.3812 s/iter. ETA=0:00:08\n",
            "[06/03 21:09:48 d2.evaluation.evaluator]: Inference done 270/271. Dataloading: 0.2566 s/iter. Inference: 0.1112 s/iter. Eval: 0.0004 s/iter. Total: 0.3686 s/iter. ETA=0:00:00\n",
            "[06/03 21:09:49 d2.evaluation.evaluator]: Total inference time: 0:01:38.046205 (0.368595 s / iter per device, on 1 devices)\n",
            "[06/03 21:09:49 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:29 (0.111170 s / iter per device, on 1 devices)\n",
            "[06/03 21:09:49 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[06/03 21:09:49 d2.evaluation.coco_evaluation]: Saving results to ./output_wind_turbine_damage/inference/coco_instances_results.json\n",
            "[06/03 21:09:49 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "[06/03 21:09:49 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[06/03 21:09:49 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.12 seconds.\n",
            "[06/03 21:09:49 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[06/03 21:09:49 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.04 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.001\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.004\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.001\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.005\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.004\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.002\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.004\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.006\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.021\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.006\n",
            "[06/03 21:09:49 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.099 | 0.438  | 0.017  | 0.078 | 0.533 | 0.355 |\n",
            "[06/03 21:09:49 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category           | AP    | category      | AP    | category   | AP    |\n",
            "|:-------------------|:------|:--------------|:------|:-----------|:------|\n",
            "| damage             | nan   | corrosion     | 0.494 | lightning  | 0.000 |\n",
            "| lightning receptor | 0.000 | missing teeth | 0.000 | patch      | 0.000 |\n",
            "[06/03 21:09:49 d2.engine.defaults]: Evaluation results for val_dataset in csv format:\n",
            "[06/03 21:09:49 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[06/03 21:09:49 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[06/03 21:09:49 d2.evaluation.testing]: copypaste: 0.0989,0.4378,0.0168,0.0782,0.5331,0.3553\n",
            "[06/03 21:09:49 d2.utils.events]:  eta: 0:15:33  iter: 999  total_loss: 0.2634  loss_cls: 0.05352  loss_box_reg: 0.02023  loss_rpn_cls: 0.03465  loss_rpn_loc: 0.1359  time: 2.0588  data_time: 0.8694  lr: 0.00024975  max_mem: 5030M\n",
            "[06/03 21:10:28 d2.utils.events]:  eta: 0:14:56  iter: 1019  total_loss: 0.3166  loss_cls: 0.07031  loss_box_reg: 0.03994  loss_rpn_cls: 0.05396  loss_rpn_loc: 0.1978  time: 2.0564  data_time: 0.9201  lr: 0.00025  max_mem: 5030M\n",
            "[06/03 21:11:11 d2.utils.events]:  eta: 0:14:21  iter: 1039  total_loss: 0.3728  loss_cls: 0.06111  loss_box_reg: 0.01852  loss_rpn_cls: 0.04567  loss_rpn_loc: 0.1794  time: 2.0582  data_time: 1.1046  lr: 0.00025  max_mem: 5030M\n",
            "[06/03 21:11:48 d2.utils.events]:  eta: 0:13:41  iter: 1059  total_loss: 0.3514  loss_cls: 0.07928  loss_box_reg: 0.0304  loss_rpn_cls: 0.0479  loss_rpn_loc: 0.2101  time: 2.0540  data_time: 0.8201  lr: 0.00025  max_mem: 5030M\n",
            "[06/03 21:12:26 d2.utils.events]:  eta: 0:13:04  iter: 1079  total_loss: 0.3676  loss_cls: 0.07143  loss_box_reg: 0.04899  loss_rpn_cls: 0.0535  loss_rpn_loc: 0.1976  time: 2.0510  data_time: 0.9136  lr: 0.00025  max_mem: 5030M\n",
            "[06/03 21:13:05 d2.utils.events]:  eta: 0:12:25  iter: 1099  total_loss: 0.3385  loss_cls: 0.07333  loss_box_reg: 0.04038  loss_rpn_cls: 0.04598  loss_rpn_loc: 0.1908  time: 2.0501  data_time: 0.9874  lr: 0.00025  max_mem: 5030M\n",
            "[06/03 21:13:51 d2.utils.events]:  eta: 0:11:48  iter: 1119  total_loss: 0.3604  loss_cls: 0.06588  loss_box_reg: 0.02428  loss_rpn_cls: 0.04484  loss_rpn_loc: 0.2042  time: 2.0543  data_time: 1.3034  lr: 0.00025  max_mem: 5030M\n",
            "[06/03 21:14:30 d2.utils.events]:  eta: 0:11:10  iter: 1139  total_loss: 0.2923  loss_cls: 0.07319  loss_box_reg: 0.0293  loss_rpn_cls: 0.03638  loss_rpn_loc: 0.1719  time: 2.0520  data_time: 0.9509  lr: 0.00025  max_mem: 5030M\n",
            "[06/03 21:15:11 d2.utils.events]:  eta: 0:10:33  iter: 1159  total_loss: 0.3128  loss_cls: 0.064  loss_box_reg: 0.03566  loss_rpn_cls: 0.04186  loss_rpn_loc: 0.1675  time: 2.0518  data_time: 0.9741  lr: 0.00025  max_mem: 5030M\n",
            "[06/03 21:15:54 d2.utils.events]:  eta: 0:09:58  iter: 1179  total_loss: 0.309  loss_cls: 0.06681  loss_box_reg: 0.03423  loss_rpn_cls: 0.04013  loss_rpn_loc: 0.1768  time: 2.0540  data_time: 1.1305  lr: 0.00025  max_mem: 5030M\n",
            "[06/03 21:16:31 d2.utils.events]:  eta: 0:09:19  iter: 1199  total_loss: 0.2841  loss_cls: 0.05872  loss_box_reg: 0.024  loss_rpn_cls: 0.04604  loss_rpn_loc: 0.1485  time: 2.0506  data_time: 0.8702  lr: 0.00025  max_mem: 5030M\n",
            "[06/03 21:17:12 d2.utils.events]:  eta: 0:08:41  iter: 1219  total_loss: 0.295  loss_cls: 0.06319  loss_box_reg: 0.02981  loss_rpn_cls: 0.04005  loss_rpn_loc: 0.177  time: 2.0505  data_time: 1.0552  lr: 0.00025  max_mem: 5030M\n",
            "[06/03 21:17:52 d2.utils.events]:  eta: 0:08:02  iter: 1239  total_loss: 0.3189  loss_cls: 0.05473  loss_box_reg: 0.008857  loss_rpn_cls: 0.04179  loss_rpn_loc: 0.1904  time: 2.0497  data_time: 1.0158  lr: 0.00025  max_mem: 5030M\n",
            "[06/03 21:18:38 d2.utils.events]:  eta: 0:07:25  iter: 1259  total_loss: 0.4068  loss_cls: 0.06806  loss_box_reg: 0.03208  loss_rpn_cls: 0.04037  loss_rpn_loc: 0.2249  time: 2.0532  data_time: 1.2254  lr: 0.00025  max_mem: 5030M\n",
            "[06/03 21:19:19 d2.utils.events]:  eta: 0:06:48  iter: 1279  total_loss: 0.2844  loss_cls: 0.06731  loss_box_reg: 0.03092  loss_rpn_cls: 0.03217  loss_rpn_loc: 0.1479  time: 2.0532  data_time: 1.0399  lr: 0.00025  max_mem: 5030M\n",
            "[06/03 21:19:58 d2.utils.events]:  eta: 0:06:10  iter: 1299  total_loss: 0.357  loss_cls: 0.0768  loss_box_reg: 0.03247  loss_rpn_cls: 0.03638  loss_rpn_loc: 0.1653  time: 2.0521  data_time: 0.9971  lr: 0.00025  max_mem: 5030M\n",
            "[06/03 21:20:43 d2.utils.events]:  eta: 0:05:33  iter: 1319  total_loss: 0.2994  loss_cls: 0.06505  loss_box_reg: 0.02058  loss_rpn_cls: 0.04315  loss_rpn_loc: 0.1285  time: 2.0546  data_time: 1.1958  lr: 0.00025  max_mem: 5030M\n",
            "[06/03 21:21:22 d2.utils.events]:  eta: 0:04:56  iter: 1339  total_loss: 0.3684  loss_cls: 0.06741  loss_box_reg: 0.03726  loss_rpn_cls: 0.0546  loss_rpn_loc: 0.2172  time: 2.0534  data_time: 0.9786  lr: 0.00025  max_mem: 5030M\n",
            "[06/03 21:22:09 d2.utils.events]:  eta: 0:04:20  iter: 1359  total_loss: 0.2562  loss_cls: 0.05175  loss_box_reg: 0.02148  loss_rpn_cls: 0.03782  loss_rpn_loc: 0.1449  time: 2.0576  data_time: 1.3438  lr: 0.00025  max_mem: 5030M\n",
            "[06/03 21:22:45 d2.utils.events]:  eta: 0:03:43  iter: 1379  total_loss: 0.2738  loss_cls: 0.05455  loss_box_reg: 0.0329  loss_rpn_cls: 0.03592  loss_rpn_loc: 0.1508  time: 2.0542  data_time: 0.8155  lr: 0.00025  max_mem: 5030M\n",
            "[06/03 21:23:29 d2.utils.events]:  eta: 0:03:06  iter: 1399  total_loss: 0.254  loss_cls: 0.05579  loss_box_reg: 0.033  loss_rpn_cls: 0.03156  loss_rpn_loc: 0.1178  time: 2.0558  data_time: 1.1245  lr: 0.00025  max_mem: 5030M\n",
            "[06/03 21:24:08 d2.utils.events]:  eta: 0:02:29  iter: 1419  total_loss: 0.1956  loss_cls: 0.04811  loss_box_reg: 0.02542  loss_rpn_cls: 0.02962  loss_rpn_loc: 0.1063  time: 2.0548  data_time: 0.9189  lr: 0.00025  max_mem: 5030M\n",
            "[06/03 21:24:48 d2.utils.events]:  eta: 0:01:53  iter: 1439  total_loss: 0.2986  loss_cls: 0.07038  loss_box_reg: 0.04304  loss_rpn_cls: 0.03129  loss_rpn_loc: 0.1382  time: 2.0540  data_time: 1.0043  lr: 0.00025  max_mem: 5030M\n",
            "[06/03 21:25:32 d2.utils.events]:  eta: 0:01:14  iter: 1459  total_loss: 0.3221  loss_cls: 0.071  loss_box_reg: 0.03937  loss_rpn_cls: 0.0409  loss_rpn_loc: 0.1516  time: 2.0556  data_time: 1.1615  lr: 0.00025  max_mem: 5030M\n",
            "[06/03 21:26:11 d2.utils.events]:  eta: 0:00:37  iter: 1479  total_loss: 0.2731  loss_cls: 0.07391  loss_box_reg: 0.03464  loss_rpn_cls: 0.03909  loss_rpn_loc: 0.1385  time: 2.0543  data_time: 0.9913  lr: 0.00025  max_mem: 5030M\n",
            "[06/03 21:26:55 d2.utils.events]:  eta: 0:00:00  iter: 1499  total_loss: 0.3931  loss_cls: 0.08089  loss_box_reg: 0.04609  loss_rpn_cls: 0.03582  loss_rpn_loc: 0.1619  time: 2.0556  data_time: 1.0870  lr: 0.00025  max_mem: 5030M\n",
            "[06/03 21:26:55 d2.engine.hooks]: Overall training speed: 1498 iterations in 0:51:19 (2.0556 s / it)\n",
            "[06/03 21:26:55 d2.engine.hooks]: Total training time: 0:53:05 (0:01:45 on hooks)\n",
            "WARNING [06/03 21:26:55 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[06/03 21:26:55 d2.data.datasets.coco]: Loaded 271 images in COCO format from /content/drive/MyDrive/Wind Turbine damage/valid/_annotations.coco.json\n",
            "[06/03 21:26:55 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[06/03 21:26:55 d2.data.common]: Serializing 271 elements to byte tensors and concatenating them all ...\n",
            "[06/03 21:26:55 d2.data.common]: Serialized dataset takes 0.18 MiB\n",
            "[06/03 21:26:55 d2.evaluation.evaluator]: Start inference on 271 batches\n",
            "[06/03 21:27:02 d2.evaluation.evaluator]: Inference done 11/271. Dataloading: 0.2195 s/iter. Inference: 0.1136 s/iter. Eval: 0.0003 s/iter. Total: 0.3334 s/iter. ETA=0:01:26\n",
            "[06/03 21:27:08 d2.evaluation.evaluator]: Inference done 20/271. Dataloading: 0.4004 s/iter. Inference: 0.1158 s/iter. Eval: 0.0009 s/iter. Total: 0.5175 s/iter. ETA=0:02:09\n",
            "[06/03 21:27:15 d2.evaluation.evaluator]: Inference done 28/271. Dataloading: 0.5289 s/iter. Inference: 0.1160 s/iter. Eval: 0.0007 s/iter. Total: 0.6468 s/iter. ETA=0:02:37\n",
            "[06/03 21:27:20 d2.evaluation.evaluator]: Inference done 42/271. Dataloading: 0.4199 s/iter. Inference: 0.1171 s/iter. Eval: 0.0007 s/iter. Total: 0.5385 s/iter. ETA=0:02:03\n",
            "[06/03 21:27:26 d2.evaluation.evaluator]: Inference done 53/271. Dataloading: 0.4066 s/iter. Inference: 0.1167 s/iter. Eval: 0.0006 s/iter. Total: 0.5246 s/iter. ETA=0:01:54\n",
            "[06/03 21:27:31 d2.evaluation.evaluator]: Inference done 62/271. Dataloading: 0.4104 s/iter. Inference: 0.1181 s/iter. Eval: 0.0008 s/iter. Total: 0.5300 s/iter. ETA=0:01:50\n",
            "[06/03 21:27:37 d2.evaluation.evaluator]: Inference done 69/271. Dataloading: 0.4459 s/iter. Inference: 0.1174 s/iter. Eval: 0.0008 s/iter. Total: 0.5648 s/iter. ETA=0:01:54\n",
            "[06/03 21:27:43 d2.evaluation.evaluator]: Inference done 84/271. Dataloading: 0.4130 s/iter. Inference: 0.1173 s/iter. Eval: 0.0007 s/iter. Total: 0.5318 s/iter. ETA=0:01:39\n",
            "[06/03 21:27:48 d2.evaluation.evaluator]: Inference done 91/271. Dataloading: 0.4389 s/iter. Inference: 0.1177 s/iter. Eval: 0.0007 s/iter. Total: 0.5581 s/iter. ETA=0:01:40\n",
            "[06/03 21:27:54 d2.evaluation.evaluator]: Inference done 105/271. Dataloading: 0.4160 s/iter. Inference: 0.1159 s/iter. Eval: 0.0007 s/iter. Total: 0.5333 s/iter. ETA=0:01:28\n",
            "[06/03 21:27:59 d2.evaluation.evaluator]: Inference done 115/271. Dataloading: 0.4130 s/iter. Inference: 0.1170 s/iter. Eval: 0.0007 s/iter. Total: 0.5313 s/iter. ETA=0:01:22\n",
            "[06/03 21:28:04 d2.evaluation.evaluator]: Inference done 139/271. Dataloading: 0.3565 s/iter. Inference: 0.1160 s/iter. Eval: 0.0006 s/iter. Total: 0.4738 s/iter. ETA=0:01:02\n",
            "[06/03 21:28:09 d2.evaluation.evaluator]: Inference done 152/271. Dataloading: 0.3508 s/iter. Inference: 0.1144 s/iter. Eval: 0.0006 s/iter. Total: 0.4664 s/iter. ETA=0:00:55\n",
            "[06/03 21:28:14 d2.evaluation.evaluator]: Inference done 164/271. Dataloading: 0.3474 s/iter. Inference: 0.1148 s/iter. Eval: 0.0006 s/iter. Total: 0.4633 s/iter. ETA=0:00:49\n",
            "[06/03 21:28:19 d2.evaluation.evaluator]: Inference done 178/271. Dataloading: 0.3398 s/iter. Inference: 0.1141 s/iter. Eval: 0.0006 s/iter. Total: 0.4551 s/iter. ETA=0:00:42\n",
            "[06/03 21:28:24 d2.evaluation.evaluator]: Inference done 199/271. Dataloading: 0.3166 s/iter. Inference: 0.1139 s/iter. Eval: 0.0006 s/iter. Total: 0.4316 s/iter. ETA=0:00:31\n",
            "[06/03 21:28:29 d2.evaluation.evaluator]: Inference done 220/271. Dataloading: 0.2985 s/iter. Inference: 0.1136 s/iter. Eval: 0.0005 s/iter. Total: 0.4132 s/iter. ETA=0:00:21\n",
            "[06/03 21:28:35 d2.evaluation.evaluator]: Inference done 240/271. Dataloading: 0.2861 s/iter. Inference: 0.1130 s/iter. Eval: 0.0005 s/iter. Total: 0.4001 s/iter. ETA=0:00:12\n",
            "[06/03 21:28:40 d2.evaluation.evaluator]: Inference done 258/271. Dataloading: 0.2782 s/iter. Inference: 0.1124 s/iter. Eval: 0.0005 s/iter. Total: 0.3916 s/iter. ETA=0:00:05\n",
            "[06/03 21:28:43 d2.evaluation.evaluator]: Total inference time: 0:01:42.286036 (0.384534 s / iter per device, on 1 devices)\n",
            "[06/03 21:28:43 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:29 (0.112064 s / iter per device, on 1 devices)\n",
            "[06/03 21:28:43 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[06/03 21:28:43 d2.evaluation.coco_evaluation]: Saving results to ./output_wind_turbine_damage/inference/coco_instances_results.json\n",
            "[06/03 21:28:43 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "[06/03 21:28:43 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[06/03 21:28:43 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.09 seconds.\n",
            "[06/03 21:28:43 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[06/03 21:28:43 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.03 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.004\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.011\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.001\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.018\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.004\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.004\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.013\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.008\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.046\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.015\n",
            "[06/03 21:28:43 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
            "| 0.377 | 1.075  | 0.045  | 0.124 | 1.780 | 0.371 |\n",
            "[06/03 21:28:43 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category           | AP    | category      | AP    | category   | AP    |\n",
            "|:-------------------|:------|:--------------|:------|:-----------|:------|\n",
            "| damage             | nan   | corrosion     | 0.886 | lightning  | 0.000 |\n",
            "| lightning receptor | 0.000 | missing teeth | 0.009 | patch      | 0.990 |\n",
            "[06/03 21:28:43 d2.engine.defaults]: Evaluation results for val_dataset in csv format:\n",
            "[06/03 21:28:43 d2.evaluation.testing]: copypaste: Task: bbox\n",
            "[06/03 21:28:43 d2.evaluation.testing]: copypaste: AP,AP50,AP75,APs,APm,APl\n",
            "[06/03 21:28:43 d2.evaluation.testing]: copypaste: 0.3769,1.0749,0.0453,0.1243,1.7800,0.3708\n",
            "Обучение завершено.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import cv2\n",
        "import torch\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "from detectron2.data import detection_utils as utils\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(classes)\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7\n",
        "\n",
        "# Создаем предиктор.\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "test_dataset_dicts = DatasetCatalog.get(\"test_dataset\")\n",
        "test_metadata = MetadataCatalog.get(\"test_dataset\")\n",
        "\n",
        "predictions_csv_path = \"predictions.csv\"\n",
        "with open(predictions_csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow([\"image_id\", \"objects\"])\n",
        "\n",
        "\n",
        "    for i, d in enumerate(test_dataset_dicts):\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Processing image {i+1}/{len(test_dataset_dicts)}\")\n",
        "        im = utils.read_image(d[\"file_name\"], format=\"BGR\")\n",
        "        outputs = predictor(im)\n",
        "\n",
        "        instances = outputs[\"instances\"].to(\"cpu\")\n",
        "        boxes = instances.pred_boxes.tensor.numpy()\n",
        "        scores = instances.scores.numpy()\n",
        "        pred_classes = instances.pred_classes.numpy()\n",
        "\n",
        "        objects_output_format = []\n",
        "        for box, score, cls_id in zip(boxes, scores, pred_classes):\n",
        "            x1, y1, x2, y2 = box\n",
        "            w = x2 - x1\n",
        "            h = y2 - y1\n",
        "            objects_output_format.append([\n",
        "                test_metadata.thing_classes[cls_id],\n",
        "                float(x1),\n",
        "                float(y1),\n",
        "                float(w),\n",
        "                float(h),\n",
        "                float(score)\n",
        "            ])\n",
        "\n",
        "        image_id = os.path.basename(d[\"file_name\"])\n",
        "\n",
        "        objects_string_for_csv = str(objects_output_format).replace(\"'\", '\"')\n",
        "        writer.writerow([image_id, objects_string_for_csv])\n",
        "\n",
        "print(f\"Inference finished. Predictions saved to: {predictions_csv_path}\")\n",
        "print(f\"Example output format in CSV for the last processed image (image_id, objects):\")\n",
        "print(f\"{image_id},{objects_string_for_csv}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl0CXt6gK2qD",
        "outputId": "35866a8b-0a16-4c97-9c22-60fea16b159b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING [06/03 21:28:56 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[06/03 21:28:56 d2.data.datasets.coco]: Loaded 271 images in COCO format from /content/drive/MyDrive/Wind Turbine damage/valid/_annotations.coco.json\n",
            "Processing image 1/271\n",
            "Processing image 101/271\n",
            "Processing image 201/271\n",
            "Inference finished. Predictions saved to: predictions.csv\n",
            "Example output format in CSV for the last processed image (image_id, objects):\n",
            "20240120_155211_mp4-0012_jpg.rf.1b3bd46c3fee8605cb7a8e69f61c4fde.jpg,[]\n"
          ]
        }
      ]
    }
  ]
}